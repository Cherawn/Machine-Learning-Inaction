# logistic回归简介 

@(机器学习)[logistic回归]

logistic回归由Cox在1958年提出，它的名字虽然叫回归，但这是一种二分类算法，并且是一种线性模型。由于是线性模型，因此在预测时计算简单，在某些大规模分类问题，如广告点击率预估（CTR）上得到了成功的应用。如果你的数据规模巨大，而且要求预测速度非常快，则非线性核的SVM、神经网络等非线性模型已经无法使用，此时logistic回归是你为数不多的选择。

--------

[toc]

-------

## 直接预测样本属于正样本的概率

logistic回归源于一个非常朴素的想法：对于二分类问题，能否直接预测出一个样本属于正样本的概率值。首先考虑最简单的情况，如果样本的输入向量是一个标量 ，如何将它映射成一个概率值。我们知道，一个随机事件的概率$p(a)$必须满足两个条件：

> 概率值是非负的，即$p(a)>=0$
> 概率值不能大于1，即$p(a)<=1$

这两个要求可以合并成，概率值必须在区间$[0,1]$内。在这里，样本的标签值为0或者1，分别代表负样本和正样本。将样本属于正样本这一事件记为$p(y =1|x)$，即已知样本的特征向量值x，其标签值属于1的条件概率，也就是样本是正样本这一事件的概率。x的取值范围可以是$(−∞ ,+∞ )$，现在想想，哪些函数能够将一个$(−∞ ,+∞ )$之内的实数值变换到区间$[0,1]$。

这就是机器学习中被广为使用的logistic函数，也叫sigmoid函数，它有一个迷人的性质，单调增，并且定义域是$(−∞ ,+∞ )$，值域是$(0,1)$（虽然不能取到0或者1，但这两个单独的点无关大局）。在神经网络的早期，也广泛的使用这个函数。

$$
f(x)=\dfrac{1}{1+e^x}
$$
现在看来，问题基本上解决了，我们已经找到了这样一个函数，输入一个样本的特征x，就可以得到一个$(0,1)$内的概率值，这就是样本属于正样本的概率。不过，之前我们假设样本的特征向量是一个标量，实际应用中不是这样的，它一般是一个向量。解决这个问题很简单，主要将向量映射成标量，然后带入logistic函数中继续预测就可以了。最简单的，可以使用线性映射如加权和：

$$
w_ 0+w_ 1·x_ 1+···+w_n·x_ 1
$$

写成向量形式为：

$$
w^Tx+b
$$

其中，$w$为权重向量，$b$为偏置项，是一个标量。至此，我们得到将一个样本的特征向量映射成一个概率值$p(y =1|x)$的函数：

$$
h(x)=\frac{1}{1+e^{-w^Tx+b}}
$$

这就是logistic回归的预测函数，至于怎样用这个映射函数做分类，接下来我们详细介绍。

----------

## 对数似然比

根据前面的定义，一个样本属于正样本的概率为：
$$
p(y=1|x)=h(x)
$$

由于不是正样本就是负样本，因此属于负样本的概率为：

$$
p(y=0|x)=1-h(x)
$$

其中$y$为类别标签，取值为1或者0，分别对应正负样本。样本属于正样本和负样本概率值比的对数称为对数似然比：

$$
log\frac{p(y=1|x)}{p(y=0|x)}=log\frac{\frac{1}{1+e^{-w^Tx+b}}}{1-\frac{1}{1+e^{-w^Tx+b}}}=w^Tx+b
$$

按照常理，分类规则为，如果如果正样本的概率大于负样本的概率，即：

$$
h(x)>0.5
$$

则样本被判定为正样本；否则被判定为负样本。而这等价于：

$$
\frac{h(x)}{1-h(x)}=\frac{p(y=1|x)}{p(y=0|x)}>1
$$

即：

$$
log\frac{p(y=1|x)}{p(y=0|x)}>log^1=0
$$

也就是下面的线性不等式：

$$
w^Tx+b>0
$$

因此logistic回归是一个线性模型。在预测时，只需要计算上面这个线性函数的值，然后和0比较即可，而不需要用logistic函数进行映射，因为概率值大于0.5与上的值大于0是等价的。logistic函数映射只用于训练时。虽然用了非线性的logistic函数，但并不能改变logistic回归是一个线性分类器的本质，因为logistic函数是一个单调增函数。

## 最大似然估计求解

前面介绍了logistic回归的预测函数与分类规则，接下来说明参数$w$和$b$是如何训练得到的。为了简化表述，在这里对向量进行扩充，将$w$和$b$合并成一个向量$w$，将向量$x$也扩充一维，其值固定为1，这样映射函数可以写成：
$$
h(x)=\frac{1}{1+e^{-w^Tx}}
$$

假设训练样本集为$(x_i,y_i ),i=1,2,...,l$，其中$x_i$是特征向量；$y_i$为类别标签，取值为1或0。给定参数$w$和样本特征向量$x$，样本属于每个类的概率可以统一写成如下形式：

$$
p(y|x,w)=h(x)^y·(1-h(x))^{1-y}
$$

证明很简单，令$y$为1和0，上式分别等于样本属于正负样本的概率。logistic回归预测的是样本属于某一类的概率，样本的类别标签为离散的1或者0，因此不适合直接用欧氏距离误差来定义损失函数，这里通过最大似然估计来确定参数。由于样本之间相互独立，训练样本集的似然函数为：

$$
L(w)= \prod_{i=1}^{l}p(y_i|x_i,w)=\prod_{i=1}^{l}(h(x_i)^{y_i}(1-h(x_i))^{1-y_i})
$$

这个函数对应于n重伯努利分布。对数似然函数为：

$$
f(w)=logL(w)=\sum_{i=1}^{l}(y_i·log^{h(x_i)}+(1-y_i)·log^{1-h(x_i)})
$$

要求该函数的最大值，等价于求解如下最小化问题：

$$
min_w\ \ (-f(w))
$$

可以使用梯度下降法求解，目标函数的梯度为：


$$
\begin{aligned}
&-\bigtriangledown \sum_{i=1}^{l}(y_ilog^{h(x_i)}+(1-y_i)log^{1-h(x_i)})\\
=&-\sum_{i=1}^{l}(y_i\frac{1}{h(x_i)}h(x_i)(1-h(x_i))x_i+(1-y_i)\frac{1}{1-h(x_i)}(-1)h(x_i)(1-h(x_i))x_i)\\
=&-\sum_{i=1}^{l}(y_i(1-h(x_i))x_i-(1-y_i)h(x_i)x_i)\\
=&\sum_{i=1}^{l}(h(x_i)-y_i)x_i
\end{aligned}
$$

> 关于第一个公式到第二个公式的推导如下：
> $$
> \frac{\partial y_ilog^{h(x_i)} }{\partial x_i}=y_i\frac{\partial log^{h(x_i)} }{\partial x_i}=y_i\frac{1}{h(x_i)}\frac{\partial {h(x_i)} }{\partial x_i}
> $$
> 又因为$h(x)=\frac{1}{1+e^{-w^Tx}}$，设$g(x)=-w^Tx$，则$h(x)=\frac{1}{1+e^{g(x)}}$
>
> 所以
> $$
> \frac{\partial {h(x_i)} }{\partial x_i}=\frac{{g(x_i)}'e^{g(x_i)}}{(1+e^{g(x_i)})^2}=h(x_i)(1-h(x_i))x_i
> $$

最后得到权重的梯度下降法的迭代更新公式为：
$$
w_{k+1}=w_k-\alpha \sum_{i=1}^{l}(h_w(x_i)-y_i)x_i
$$

>### 梯度上升法与梯度下降法区别
>
>梯度上升法求取函数的最大值，而梯度下降法求取函数的最小值。
>但两种方法可以转化：$max_w\ \ f(w)=min_w\ \ (-f(w))$
>因此根据梯度下降法公式：$w_{k+1}=w_k-\alpha\bigtriangledown f(w)$，因为$\bigtriangledown -f(w)=-\bigtriangledown f(w)$，所以梯度上升法公式为：$w_{k+1}=w_k+\alpha\bigtriangledown f(w)$。
